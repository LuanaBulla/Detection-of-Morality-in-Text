# Detection-of-Morality-in-Text

### Table of contents
* [General Information](#general-information)
* [Supervised Models](#Supervised)
* [Unsupervised Models](#Unsupervised)
* [Results](#Results)

### General Information
The theory of moral foundations (MFT) is a socio-cognitive psychological theory that constitutes a general framework aimed at explaining the origin and evolution of human moral reasoning. Because of its dyadic structure of values and their violations, it can be used as a theoretical background for discerning moral values from natural language text as it captures a user's perspective on a specific topic. In this repository, we offer tools to perform an automatic detection of moral content in sentences or short paragraphs using machine learning techniques.

### Supervised Models
- The [MFTC Classifier.py](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/blob/main/MoralClassifier.py) is a Bert-based multi-label classifier fine-tuned on the [Moral Foundation Twitter Corpus (MFTC)](https://journals.sagepub.com/doi/full/10.1177/1948550619876629), a dataset of tweets annotated with MFT labels. For additional details, please refer to [Detection of Morality in Tweets Based on the Moral Foundation Theory](https://link.springer.com/chapter/10.1007/978-3-031-25599-1_1).
- The [MFRC Classifier.py](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/blob/main/MFRC%20Classifier.py) is a Bert-based multi-label classifier fine-tuned on the [Moral Foundation Reddit Corpus (MFRC)](https://arxiv.org/abs/2208.05545). This dataset comprises Reddit comments annotated with MFT foundations. For more comprehensive information, please consult the paper [Do Language Models Understand Morality? Towards a Robust Detection of the Moral Content]().

### Unsupervised Models
- The [NLI_0shot.py](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/blob/main/NLI_0shot.py) is a zero-shot multi-label classifier built upon the MFT labels using a Natural Language Inference (NLI) system. This NLI model indirectly acquires common-sense knowledge, enabling it to detect core values in the input text solely through semantic interpretation based on MFT label taxonomies. For a more comprehensive understanding of the methodology, please consult references [Uncovering values: detecting latent moral content from natural language with explainable and non-trained methods](https://aclanthology.org/2022.deelio-1.4/), [Do Language Models Understand Morality? Towards a Robust Detection of the Moral Content](https://books.google.it/books?hl=it&lr=&id=OpkbEQAAQBAJ&oi=fnd&pg=PA98&dq=do+language+model+understand+morality%3F&ots=8WQioeGC-W&sig=0sIN3A6ljXdEHj15oZxajsdr_r8&redir_esc=y#v=onepage&q=do%20language%20model%20understand%20morality%3F&f=false) and [Large Language Models Meet Moral Values: a
Comprehensive Assessment of Moral Abilities]().
- The [LLMs.py](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/blob/main/LLMs.py) prompts different large language models (LLMs) by treating them as zero-shot, unsupervised, multi-label moral value classifiers. The code supports two types of prompting scenarios that delve into the tasks of binary Classification (i.e. determines whether the content contains moral content or not) and multi-Label classification (i.e. classifies content into multiple moral value categories based on the MFT taxonomy). For further details, refer to Paper Large Language Models Meet Moral Values: a
Comprehensive Assessment of Moral Abilities]().

### Results
The [Results](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/tree/main/Results) contains the outcomes of the NLI-based zero-shot approach applied to the subcorpora of the MFRC, compared with results from various LLMs of differing sizes. The folder is organized into two subdirectories:
1. [Exp.1](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/tree/main/Results/Exp.1) that includes files detailing predictions of moral values generated by the Da Vinci GPT-3 model. It also contains moral labels assigned using supervised approaches, specifically through multi-label RoBERTa classifiers. For additional information, please refer to [Do Language Models Understand Morality? Towards a Robust Detection of the Moral Content](https://books.google.it/books?hl=it&lr=&id=OpkbEQAAQBAJ&oi=fnd&pg=PA98&dq=do+language+model+understand+morality%3F&ots=8WQioeGC-W&sig=0sIN3A6ljXdEHj15oZxajsdr_r8&redir_esc=y#v=onepage&q=do%20language%20model%20understand%20morality%3F&f=false).
2. [Exp.2](https://github.com/LuanaBulla/Detection-of-Morality-in-Text/tree/main/Results/Exp.2) that contains the results of experiments evaluating the performance of LLMs in zero-shot and few-shot multi-label settings. The experiments compare the capabilities of GPT-4, GPT-3.5, Llama-70B, Llama-7B, Mixtral-7x6B, and Mistral-7B for multi-label classification of moral content, tested under two distinct prompting configurations: single-prompt and double-prompt.
In addition, this subdirectory includes results from an innovative experimental framework that assesses these models' performance against human judgments in a head-to-head comparison. This framework offers valuable insights into the relative effectiveness of LLMs and human annotators in the task of moral content classification. For further details, consult [Large Language Models Meet Moral Values: a
Comprehensive Assessment of Moral Abilities]().


  
